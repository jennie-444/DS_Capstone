{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d40e2f-d640-4f92-98f5-01c734063a35",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06e56a-a39f-4a19-804c-1af86add88fc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b61fc9-9246-4bf8-b9a1-d82c37a66db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "from functions import train_model, load_data, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d19d92-0423-45d4-b689-e27522dd07ff",
   "metadata": {},
   "source": [
    "## CNN structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448266b2-1f48-4935-9405-79bb066011ef",
   "metadata": {},
   "source": [
    "### Choices in this version:\n",
    "\n",
    "1. Four convolutional layers helps learn more features from the images, but risks overfitting\n",
    "2. Global average pooling pools across entire feature maps to one value but not locally, which although losing spatial information, may work sufficiently for classification and avoid overfitting\n",
    "3.  Batch normalization helps standardize outputs of batches by normalizing, helps network run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0662d49-dc79-489e-8680-03ef6f173077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI_CNN(nn.Module):\n",
    "    '''This version of our CNN model includes Global average pooling, four convolutional layers, batch normalization\n",
    "    '''\n",
    "    def __init__(self, num_classes, dropout = 0.5):\n",
    "        super(MRI_CNN, self).__init__()\n",
    "        \n",
    "        # 4 convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # Reduces spatial dimensions\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Dropout helps avoid overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.global_avg_pool(x) \n",
    "        \n",
    "        x = x.view(x.size(0), -1 #flatten before feeding to fc layer\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722c70a-dc6f-4f86-b49e-a6707a37d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training on CNN image classifier\n",
    "def run_cnn(\n",
    "    batch_size, save_dir, dropout, lr, epochs\n",
    "):\n",
    "    NUM_CLASSES = 4\n",
    "\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "    \n",
    "    # instantiate the model\n",
    "    model = MRI_CNN(num_classes=NUM_CLASSES, dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    trained_model = train_model(save_dir, model, train_loader, device, optimizer, criterion, epochs)\n",
    "    \n",
    "    # run evaluation\n",
    "    evaluate_model(trained_model, test_loader, device, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570631c0-43bd-4982-8505-1b261a94b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search\n",
    "# Note: Change values below to be desired save path and search parameters\n",
    "SAVE_PATH = \"model_results/Cnn_\"\n",
    "DROPOUT = [0.1, 0.3, 0.5]\n",
    "LR = [0.001, 0.0001]\n",
    "EPOCHS = [10, 50]\n",
    "BATCH_SIZE = [32]\n",
    "\n",
    "\n",
    "COUNT = 1\n",
    "\n",
    "for i in DROPOUT:\n",
    "    for j in LR:\n",
    "        for k in EPOCHS:\n",
    "            for n in BATCH_SIZE:\n",
    "                # make save directory if it does not exist\n",
    "                SAVE_DIR = os.path.join(SAVE_PATH, str(COUNT))\n",
    "                if not os.path.exists(SAVE_DIR):\n",
    "                    os.makedirs(SAVE_DIR)\n",
    "                # train and eval, save results\n",
    "                run_cnn(n, SAVE_DIR, i, j, k)\n",
    "                # save model configuration\n",
    "                config = {\n",
    "                    \"dropout\": i,\n",
    "                    \"learning_rate\": j,\n",
    "                    \"epochs\": k,\n",
    "                    \"batch_size\": n,\n",
    "                    \"balanced\": False,\n",
    "                    \"data_preprocessing\": None\n",
    "                }\n",
    "                with open(os.path.join(SAVE_DIR, \"config.json\"), \"w\") as json_file:\n",
    "                    json.dump(config, json_file, indent=4)\n",
    "                COUNT+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oomf",
   "language": "python",
   "name": "oomf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
